{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Conversational AI: Data Science (UCS663)\n**Kaggle Based Lab Evaluation - 1** <br>\nSuvrat Arora <br>\n101903331 <br>\n3CO13 <br>","metadata":{}},{"cell_type":"markdown","source":"# Speaker Recognition \n\nIn the Speaker Recognition problem, we are provided with a dataset comprising speeches of prominent personalities in the form of .wav files. Besides, a background noise audio file has also been given, which can be intertwined with the speech audios to ensure better results. <br>\nHere, our objective is to train a model with the best possible accuracy that can correctly predict the speaker of the speech; thus, it is an **Audio Classification** problem.\n\nWe will deal with the problem statement in the following stepwise manner: <br>\n\n**- Data Exploration and Visualization:** As should be done with any Machine Learning/Data Science problem, we begin with understanding the data. Since we are given the data in directories in the form of wav audio file - we'll traverse through the directory and plot a few audio signals to understand the nature of the data provided.\n\n**- Data Pre-Processing:** After having explored the data, we'll pre-process the audio files. One of the most common ways to deal with audio data is to convert it into some kind of wave plot - thereby converting the audio processing problem into an image processing problem. Furthermore, we'll also store the wav file paths from the directories to local variables and then divide the data into training and testing datasets so that they can be employed for model training and evaluation.<br>\n\n**- Model Building:** After having pre-processed the data, the next step will be to build a model. Since we are endeavouring to convert the audio files into graphical representations, we'll need to create a Convolutional Neural Network for this purpose. Thus, we will feed the model with waveplots, apply the convolution operation to the generated graphs, and perform the speaker recognition task.<br>\n\n**- Model Evaluation:** Once the model is built on the training data set with an acceptable level of training accuracy, the model will be evaluated on the testing data, and the final accuracy will be determined. <br>\n\nSubsequently, in this notebook, we'll attempt to perform the above-mentioned steps and get the best possible results.","metadata":{}},{"cell_type":"code","source":"# Installing spela for Melspectogram\n!pip install spela","metadata":{"execution":{"iopub.status.busy":"2022-03-16T16:46:59.114262Z","iopub.execute_input":"2022-03-16T16:46:59.115117Z","iopub.status.idle":"2022-03-16T16:47:10.762816Z","shell.execute_reply.started":"2022-03-16T16:46:59.115007Z","shell.execute_reply":"2022-03-16T16:47:10.761747Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# Importing necessary libraries \nimport os\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport pathlib\nimport librosa\nimport librosa.display #Audio Processing Library for Python\nimport IPython.display as ipd\nfrom tqdm import tqdm #To form the loading bar for loading the dataset (since the dataset is quite haevy, it helps monitor the )\nimport cupy #RAPIDS accelerated alternative to numpy \nimport cudf #RAPIDS accelerated alternative to pandas \nfrom cuml.metrics import confusion_matrix\nfrom cuml.model_selection import train_test_split\nimport numpy as np #RAPIDS accelerated alternative to numpy \nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom spela.melspectrogram import Melspectrogram","metadata":{"execution":{"iopub.status.busy":"2022-03-16T16:48:26.059342Z","iopub.execute_input":"2022-03-16T16:48:26.059929Z","iopub.status.idle":"2022-03-16T16:48:26.065408Z","shell.execute_reply.started":"2022-03-16T16:48:26.059886Z","shell.execute_reply":"2022-03-16T16:48:26.064809Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"# Data Explorartion and Visualization\n Now that we have imported all the necessary libraries, we'll load our audio data, set the contents of the directory and plot the waveplots and spectrograms of one audio sample in order to get to know our data better.","metadata":{}},{"cell_type":"code","source":"# Saving the input data path \ndata_path = '../input/speaker-recognition-dataset/16000_pcm_speeches/'","metadata":{"execution":{"iopub.status.busy":"2022-03-16T16:48:36.013257Z","iopub.execute_input":"2022-03-16T16:48:36.013783Z","iopub.status.idle":"2022-03-16T16:48:36.017745Z","shell.execute_reply.started":"2022-03-16T16:48:36.013714Z","shell.execute_reply":"2022-03-16T16:48:36.017028Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# Checking the folder names present in the data folder\n# The folder names represent the speaker names\nos.listdir(data_path)","metadata":{"execution":{"iopub.status.busy":"2022-03-16T16:48:37.770436Z","iopub.execute_input":"2022-03-16T16:48:37.770817Z","iopub.status.idle":"2022-03-16T16:48:37.786148Z","shell.execute_reply.started":"2022-03-16T16:48:37.770777Z","shell.execute_reply":"2022-03-16T16:48:37.784933Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# Loading the first audio file\naudio_path = '../input/speaker-recognition-dataset/16000_pcm_speeches/Benjamin_Netanyau/0.wav'\nlibrosa_audio_data,librosa_sample_rate=librosa.load(audio_path)\n\n# Plotting the audio signal\nplt.figure(figsize=(12, 4))\nplt.plot(librosa_audio_data)","metadata":{"execution":{"iopub.status.busy":"2022-03-16T16:48:39.688207Z","iopub.execute_input":"2022-03-16T16:48:39.688542Z","iopub.status.idle":"2022-03-16T16:48:41.019837Z","shell.execute_reply.started":"2022-03-16T16:48:39.688504Z","shell.execute_reply":"2022-03-16T16:48:41.018650Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"S=librosa.feature.melspectrogram(y=librosa_audio_data, sr=librosa_sample_rate)\nplt.figure(figsize=(10, 4))\nlibrosa.display.specshow(librosa.power_to_db(S, ref=np.max), y_axis='mel', fmax=8000, x_axis='time')\nplt.colorbar(format='%+2.0f dB')\nplt.title('Mel pectrogram')\nplt.tight_layout()","metadata":{"execution":{"iopub.status.busy":"2022-03-16T16:48:43.035830Z","iopub.execute_input":"2022-03-16T16:48:43.036154Z","iopub.status.idle":"2022-03-16T16:48:43.404225Z","shell.execute_reply.started":"2022-03-16T16:48:43.036123Z","shell.execute_reply":"2022-03-16T16:48:43.403001Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"# Creating/Processing Training Dataset","metadata":{}},{"cell_type":"code","source":"# Defining a function to fetch the wav file paths\ndef get_wav_paths(speaker):\n    speaker_path = data_path + speaker\n    all_paths = [item for item in os.listdir(speaker_path)]\n    return all_paths","metadata":{"execution":{"iopub.status.busy":"2022-03-16T16:48:45.939346Z","iopub.execute_input":"2022-03-16T16:48:45.939712Z","iopub.status.idle":"2022-03-16T16:48:45.945565Z","shell.execute_reply.started":"2022-03-16T16:48:45.939672Z","shell.execute_reply":"2022-03-16T16:48:45.944794Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# Retriving the file paths to individual speakers\nnelson_mandela_paths = get_wav_paths(\"Nelson_Mandela\")\nmargaret_thatcher_paths = get_wav_paths(\"Magaret_Tarcher\")\nbenjamin_netanyau_paths = get_wav_paths(\"Benjamin_Netanyau\")\njens_stoltenberg_paths = get_wav_paths( 'Jens_Stoltenberg')\njulia_gillard_paths = get_wav_paths(\"Julia_Gillard\")","metadata":{"execution":{"iopub.status.busy":"2022-03-16T16:48:47.583959Z","iopub.execute_input":"2022-03-16T16:48:47.584425Z","iopub.status.idle":"2022-03-16T16:48:48.851004Z","shell.execute_reply.started":"2022-03-16T16:48:47.584389Z","shell.execute_reply":"2022-03-16T16:48:48.850180Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# Loading the data for model training\ndef load_wav(wav_path, speaker):\n    with tf.compat.v1.Session(graph=tf.compat.v1.Graph()) as sess:\n        wav_path = data_path +speaker + \"/\"+ wav_path\n        wav_filename_placeholder = tf.compat.v1.placeholder(tf.compat.v1.string, [])\n        wav_loader = tf.io.read_file(wav_filename_placeholder)\n        wav_decoder = tf.audio.decode_wav(wav_loader, desired_channels=1)\n        wav_data = sess.run(\n            wav_decoder, feed_dict={\n                wav_filename_placeholder: wav_path\n            }).audio.flatten().reshape((1, 16000))\n        sess.close()\n    return wav_data","metadata":{"execution":{"iopub.status.busy":"2022-03-16T16:48:52.737120Z","iopub.execute_input":"2022-03-16T16:48:52.737590Z","iopub.status.idle":"2022-03-16T16:48:52.744452Z","shell.execute_reply.started":"2022-03-16T16:48:52.737554Z","shell.execute_reply":"2022-03-16T16:48:52.743837Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# Create training data\ndef generate_training_data(speaker_paths, speaker, label):\n    wavs, labels = [], []\n    for i in tqdm(speaker_paths):\n        wav = load_wav(i, speaker)\n        wavs.append(wav)\n        labels.append(label)\n    return wavs, labels","metadata":{"execution":{"iopub.status.busy":"2022-03-16T16:48:54.319909Z","iopub.execute_input":"2022-03-16T16:48:54.320366Z","iopub.status.idle":"2022-03-16T16:48:54.325150Z","shell.execute_reply.started":"2022-03-16T16:48:54.320332Z","shell.execute_reply":"2022-03-16T16:48:54.324476Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"nelson_mandela_wavs, nelson_mandela_labels = generate_training_data(nelson_mandela_paths, \"Nelson_Mandela\", 0) \nmargaret_thatcher_wavs, margaret_thatcher_labels = generate_training_data(margaret_thatcher_paths, \"Magaret_Tarcher\", 1) \nbenjamin_netanyau_wavs, benjamin_netanyau_labels = generate_training_data(benjamin_netanyau_paths, \"Benjamin_Netanyau\", 2) \njens_stoltenberg_wavs, jens_stoltenberg_labels = generate_training_data(jens_stoltenberg_paths, \"Jens_Stoltenberg\", 3) \njulia_gillard_wavs, julia_gillard_labels = generate_training_data(julia_gillard_paths, \"Julia_Gillard\", 4) ","metadata":{"execution":{"iopub.status.busy":"2022-03-16T16:48:55.857868Z","iopub.execute_input":"2022-03-16T16:48:55.858203Z","iopub.status.idle":"2022-03-16T16:50:36.131842Z","shell.execute_reply.started":"2022-03-16T16:48:55.858165Z","shell.execute_reply":"2022-03-16T16:50:36.130831Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# remove the extra wav for Julia Gillard\njulia_gillard_labels = julia_gillard_labels[1:]\njulia_gillard_wavs = julia_gillard_wavs[1:]","metadata":{"execution":{"iopub.status.busy":"2022-03-16T16:51:37.058472Z","iopub.execute_input":"2022-03-16T16:51:37.058864Z","iopub.status.idle":"2022-03-16T16:51:37.064131Z","shell.execute_reply.started":"2022-03-16T16:51:37.058825Z","shell.execute_reply":"2022-03-16T16:51:37.062999Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"all_wavs = nelson_mandela_wavs + margaret_thatcher_wavs + benjamin_netanyau_wavs + jens_stoltenberg_wavs + julia_gillard_wavs\nall_labels = nelson_mandela_labels + margaret_thatcher_labels + benjamin_netanyau_labels + jens_stoltenberg_labels + julia_gillard_labels","metadata":{"execution":{"iopub.status.busy":"2022-03-16T16:51:39.997694Z","iopub.execute_input":"2022-03-16T16:51:39.998048Z","iopub.status.idle":"2022-03-16T16:51:40.004054Z","shell.execute_reply.started":"2022-03-16T16:51:39.998012Z","shell.execute_reply":"2022-03-16T16:51:40.003200Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# split the dataset into trainin and testing set\ntrain_wavs, test_wavs, train_labels, test_labels = train_test_split(all_wavs, all_labels, test_size=0.2)","metadata":{"execution":{"iopub.status.busy":"2022-03-16T16:51:43.858835Z","iopub.execute_input":"2022-03-16T16:51:43.859931Z","iopub.status.idle":"2022-03-16T16:51:43.869558Z","shell.execute_reply.started":"2022-03-16T16:51:43.859874Z","shell.execute_reply":"2022-03-16T16:51:43.868777Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"train_x, train_y = np.array(train_wavs), np.array(train_labels)\ntest_x, test_y = np.array(test_wavs), np.array(test_labels)","metadata":{"execution":{"iopub.status.busy":"2022-03-16T16:51:51.240195Z","iopub.execute_input":"2022-03-16T16:51:51.241010Z","iopub.status.idle":"2022-03-16T16:51:51.488461Z","shell.execute_reply.started":"2022-03-16T16:51:51.240968Z","shell.execute_reply":"2022-03-16T16:51:51.487236Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"**Mel Spectrogram** plots spectrograms by employing Mel scale - which takes into consideration the dynamics of human speech and the range at which humans are accustomed to hearing. Thus, we are using Mel spectrograms to train our CNN mode","metadata":{}},{"cell_type":"code","source":"def create_model():\n    model = tf.keras.Sequential()\n    model.add(Melspectrogram(sr=16000, n_mels=128,n_dft=512, n_hop=256,\n                            input_shape=(1 , 16000),return_decibel_melgram=True,\n                            trainable_kernel=False, name='melgram'))\n   \n\n    model.add(tf.keras.layers.Conv2D(64, (3, 3), activation=\"relu\"))\n    model.add(tf.keras.layers.MaxPool2D(pool_size=(2, 2)))\n\n    model.add(tf.keras.layers.Flatten())\n    model.add(tf.keras.layers.Dense(5, activation=\"softmax\"))\n    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=3e-4)\n            , loss = \"sparse_categorical_crossentropy\"\n            , metrics = [\"accuracy\"])\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-03-16T16:52:03.587227Z","iopub.execute_input":"2022-03-16T16:52:03.587538Z","iopub.status.idle":"2022-03-16T16:52:03.596769Z","shell.execute_reply.started":"2022-03-16T16:52:03.587506Z","shell.execute_reply":"2022-03-16T16:52:03.595791Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"# melspectrogram model\nmodel = create_model()","metadata":{"execution":{"iopub.status.busy":"2022-03-16T16:52:06.334025Z","iopub.execute_input":"2022-03-16T16:52:06.334332Z","iopub.status.idle":"2022-03-16T16:52:07.123022Z","shell.execute_reply.started":"2022-03-16T16:52:06.334297Z","shell.execute_reply":"2022-03-16T16:52:07.121864Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"model.fit(x=train_x, y=train_y, epochs=10)","metadata":{"execution":{"iopub.status.busy":"2022-03-16T16:52:34.949494Z","iopub.execute_input":"2022-03-16T16:52:34.950058Z","iopub.status.idle":"2022-03-16T17:02:57.224647Z","shell.execute_reply.started":"2022-03-16T16:52:34.950025Z","shell.execute_reply":"2022-03-16T17:02:57.223858Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"We can see that a training accuracy of **97.03%** has been achieved. Now, let us evaluate our model on the test data set and find the accuracy of it.","metadata":{}},{"cell_type":"code","source":"model.evaluate(x=test_x, y=test_y)","metadata":{"execution":{"iopub.status.busy":"2022-03-16T17:03:52.607538Z","iopub.execute_input":"2022-03-16T17:03:52.608383Z","iopub.status.idle":"2022-03-16T17:03:56.127270Z","shell.execute_reply.started":"2022-03-16T17:03:52.608341Z","shell.execute_reply":"2022-03-16T17:03:56.125978Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"Upon evaluating the model on the test data, it is clear that we have achieved an accuracy of **93.20%**","metadata":{}},{"cell_type":"markdown","source":"# Conclusion\n\nWe have sucessfully fetched the audio files from the directories, pre-processed them to form Mel Spectrograph - hereafter the problem turned into an image processing problem. Since CNNs are one of the best tools to deal with training of image models, we trained a CNN and accomplished the following accuracy levels: <br>\n\n**Training Accuracy:** 97.03% <br>\n**Testing Accuracy:** 93.20% <br>\n\n[**Loss Function used:** sparse_categorical_crossentropy <br>\n**Evaluation Metric used:** accuracy]","metadata":{}}]}